{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code used for Decoding\n",
    "### Documentation created by Ana Vedoveli in 24 Feb 2017\n",
    "\n",
    "This code is the first code used for decoding. It is divided in four principal parts (whole-trial decoding of conditions, whole trial decoding of directions, time-decoding of conditions and time-decoding of directions), in which the classifier used is always a linear support vector machine and I would try to replicate the protocol in JR King's papers. It also contains *extra* parts (not necessarily extra, but for the lack of a better name I called like this), in which I tried new classifiers or techniques to address the same data. \n",
    "\n",
    "Over this documentation, I'll provide a tutorial and try to explain what each part of the code is doing. Hopefully this will make the generalization and improvement of this code easier. I will also highlight parts in which I used other people's example, with links for them.\n",
    "\n",
    "\n",
    "Is important to know that in this tutorial we are using the **struct_cor** data. The struct_cor contains the data that was filtered and baseline corrected. The same steps can be used when decoding the **struct_inter**, the data that was filtered, baseline corrected and had the missing channels interpolated. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing toolboxes and creating it's own functions.\n",
    "\n",
    "In python, we import the functions of the different toolboxes we are using throughout the whole code. This is a similar idea to other languages, like R! and LaTeX. My personal preference is to import all the functions in the beginning of the code. That is what this next part is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jan 24 09:11:13 2018\n",
    "\n",
    "@author: Ana\n",
    "\n",
    "CI: time decoding of directions and conditions\n",
    "\"\"\"\n",
    "\n",
    "## Importing packages and defining functions.\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import (loadmat, savemat)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import (SlidingEstimator, GeneralizingEstimator,\n",
    "                          cross_val_multiscore, LinearModel, get_coef)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mne.decoding import LinearModel\n",
    "from mne.decoding import get_coef\n",
    "import mne\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import metrics\n",
    "from scipy import interp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining your own particular functions\n",
    "\n",
    "Similarly to R! and Matlab, you can create your own functions in Python. Differently from MatLab, however, you functions can be defined throught your code (another option is to have another .py file only with all your functions together). Python can have different consoles (like a command line), which means that if you open a new python console/window, you have to define the functions again. My personal preference also was to define all my own functions in the begining of the code. But this, as always, is a personal preference. \n",
    "\n",
    "Here I am defining two functions: the function **scorer**, taken from the JR King tutorials, and the function **load_cor**. The function **scorer** is responsible to scoring an AUC value over the probabilistic output of the SVM. The scoring of an AUC value *on the probabilistic output* is something that was not naturally implemented in the scikit-learn package: Usually, on scikit learn, if you ask your SVM function to give as an output a AUC value, this AUC would be automatically computed on the categorical output of the SVM, and not over the probabilistic output. Therefore, we needed to implement our own scorer and implement it on the cross-validation step.\n",
    "\n",
    "The function **load_cor** is probably the most important function we have so far. My biggest problem was: how could I export a structure in the format of EPOCHS to the MNE? [There is a good page on the Fieldtrip website explaining how to export data from Fieldtrip to MNE](http://www.fieldtriptoolbox.org/development/integrate_with_mne), however this is still in development: you can easily export data from fieldtrip to MNE if they are on the format of RAW or EVOKED, but not EPOCHS (the format of the cleaned data). For a long time in my lb rotation, it was really hard for me to import data from fieldtrip to Python in a way I could keep all the important features I wanted (the data, the time window, the fs, etc) and to be adapted to the MNE-workflow. After trying many codes that did not succeed, I finally found someone who had the same problem as mine. I could adapt their codes with what I had tried so far. This function is, in my opinion, really important -- so if you haven't read the documentation about this function, you should do it before keep reading this one. Importantly, this code is supposed to run on Windows. A small adaptation of function load_cor was created for mac users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scorer(y_true, y_pred):\n",
    "    # Probabilistic estimates are reported for each class. In our case \n",
    "    # `y_pred` shape is (n_trials, 2), where `y[:, 0] = 1 - y[:, 1]`.\n",
    "    return roc_auc_score(y_true, y_pred[:, 1])    \n",
    "\n",
    "## This code is meant to work on WINDOWS. Giulia has a version that works on MAC. \n",
    "def load_cor(xDir, var_name='struct_cor'):\n",
    "    import mne\n",
    "    from mne import create_info\n",
    "    from mne.epochs import EpochsArray\n",
    "    import scipy.io as sio\n",
    "    import numpy as np\n",
    "    # load Matlab/Fieldtrip data\n",
    "    mat = sio.loadmat(xDir, squeeze_me=True, struct_as_record=False)\n",
    "    ft_data = mat[var_name]\n",
    "    event = ft_data.trialinfo[:, 1]\n",
    "\n",
    "    # convert to mne\n",
    "    n_trial, n_chans, n_time = ft_data.trial.shape\n",
    "    data = np.zeros((n_trial, n_chans, n_time))\n",
    "    data = ft_data.trial\n",
    "\n",
    "    sfreq = 200\n",
    "    time = ft_data.time\n",
    "\n",
    "    \n",
    "    coi = range(n_chans)\n",
    "    data = data[:, coi, :]\n",
    "    chan_names = [l.encode('ascii') for l in ft_data.label[coi]]\n",
    "    chan_types = ft_data.label[coi]\n",
    "    chan_types[:] = 'eeg'\n",
    "    info = create_info(chan_names, sfreq, chan_types)\n",
    "    events = np.array([np.arange(n_trial), np.zeros(n_trial), event], int).T\n",
    "    epochs = EpochsArray(data, info, events=events,\n",
    "                         tmin=np.min(time), verbose=False)\n",
    "    montage = mne.channels.read_montage('GSN-HydroCel-257')\n",
    "    epochs.set_montage(montage)\n",
    "    return epochs, ft_data.trialinfo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the decoding script properly.\n",
    "\n",
    "This is the start of the code per se. An important thing to note here is that I would personally prefer to have each step of the decoding pipeline to run over all the subjects I had. So, first, I define a *main directory*, in which I have all subject data. In the main directory, each subject has its own folder (named by the name of the subject), and inside of this folder I have the data I will use over the . \n",
    "\n",
    "After this, I use the listdir function (from the package os) to read the name of all the folders I have inside of my main directory (in other words, the name of the subjects), and I sort them alphabetically. I also try to select only the subjects cleaned with the third method (v03).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Changing the main directory of your files.\n",
    "mainDir = 'C:\\\\Users\\\\Ana\\\\Desktop\\\\CI\\\\Python\\\\Subjects\\\\' # Change this to your own path\n",
    "\n",
    "# Start: making list of subject names.\n",
    "subject = os.listdir(mainDir)\n",
    "subject = np.sort(subject) # variable containing a list of subjects in our folder.\n",
    "\n",
    "## Selecting subjects only for the version 03 of cleaning. Skip this step if want\n",
    "# to go through all subjects.\n",
    "\n",
    "#temp =[]\n",
    "#for name in subject:\n",
    "#    if name[-3:] == 'v03':\n",
    "#        temp.append(name)\n",
    "#subject = np.asarray(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now that we have our own list of subjects, we will start the decoding properly.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  First part of analysis: decoding whole-trial of conditions\n",
    "\n",
    "Now we will start with the easiest question to answer: if we look at the whole trial (n_channels x n_time points), can we distinguish the trials in which the condition was same and when it was different? This decoding step yields a single performance value for each subject. Remember that here we will loop through all the subjects we have on our \"subject\" variable. The following code is commented to help following the logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "# notice that in python indentation is important. Here we start a for loop, \n",
    "# over every name in the subject list.\n",
    "for name in subject:\n",
    "    \n",
    "\t# In python, you have to pre-name all variables that you are going to use\n",
    "\t# beforehand. That's what we are doing here.\n",
    "    score_conditions = [] # in this variable, we have the cross-validated \n",
    "                          # AUC value (average across folds).\n",
    "        \n",
    "    score_tprs =[] # in this variable, we have the cross validated\n",
    "                   # True positive rate (average across folds).\n",
    "    \n",
    "    \n",
    "    filePath = mainDir + name # main path for the subject\n",
    "    #loading labels for conditions - we have a variable trl_conditions\n",
    "    # containing the labels (same: -1, different: 1) for each trial of \n",
    "    # that subject. \n",
    "    yDir = filePath + '\\\\trl_conditions.mat' # directory of the file\n",
    "    Y = loadmat(yDir) # loading the file with function loadmat\n",
    "    conditions = Y['trl_conditions'] # loadmat gives us many things we don't need \n",
    "                                     # (including the header of the \n",
    "                                     # file. we retrieve only the data we want.\n",
    "        \n",
    "    Y = conditions.transpose().ravel() # we rewrite variable Y with only the data \n",
    "                                       # we want (that is stored in conditions). \n",
    "                                       # We transpose the data and use the ravel method to get \n",
    "                                       # an one dimensional vector. This happens because the SVM\n",
    "                                       # function only accepts one dimmensional vectors as labels\n",
    "    \n",
    "    # In python, all these steps could have been done in only one line. \n",
    "    # However, I kept it this way so we can  follow the logic easily. \n",
    "    # I also change all the value from -1 to 0. As I understood the idea of \n",
    "    # scoring an AUC# over the probabilistic output, it would make more sense that the\n",
    "    # labels were 0 and 1 instead of 1 and -1. Another way of solving this for good is \n",
    "    # to change the value from -1 to 0 in the matlab function \"CI_defineCondition\".\n",
    "        \n",
    "    Y[Y==-1] = 0\n",
    "    \n",
    "    # Now we are going to load the EEG data as epoch object\n",
    "    xDir = mainDir + name + '\\\\struct_cor.mat'\n",
    "    epochs, _ = load_cor(xDir, var_name='struct_cor')\n",
    "    \n",
    "    # OBS: the epochs object in python is known as \"class\". A class is a type of object \n",
    "    # in python, that has functions in-built. This means, nothing more, that a class is\n",
    "    # a special variable that has its own functions  (in python we call these functions\n",
    "    # methods). Note that \"epochs\" won't show in the variable explorer in spyder. To check \n",
    "    # if you loaded your data properly, type epochs in command line and see what you get.\n",
    "    \n",
    "    #Retrieving data as matrix\n",
    "    data = epochs.get_data() # here is one example of a method (special funtion) - \n",
    "                             # get_data() gets the EEG data inside the epoch class.\n",
    "    \n",
    "    X = np.reshape(data, [data.shape[0], data.shape[1]*data.shape[2]]) # what we are you doing here is \n",
    "                                                                       # putting all the channels \n",
    "                                                                       # and time points as features for the classifier.\n",
    "    \n",
    "    ## classification problem \"conditions\".\n",
    "    \n",
    "    # Now we will define all the objects that are import for us. We will define a classifier\n",
    "    # clf, that will perform thefollowing steps: it will scale the data, perform a dimensionality \n",
    "    # reduction (selectkbest) keeping the best 20 features  and, in the end, it will fit a SVM \n",
    "    # (SVC - Support vector classifier). \n",
    "    clf = Pipeline([('scaling', StandardScaler()), ('SelectKBest', SelectKBest(f_classif, k=20)), ('svc',  SVC(class_weight='balanced', probability=True, kernel='linear'))])\n",
    "    \n",
    "    # Now we are also defining an object for the stratified cross-validation 'cv'. \n",
    "    # Object cv will split the data in 5 stratified\n",
    "    # folds, shuffling it to avoid any carry-over effect\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    # to fit your classifier for the data, you use the method 'fit':\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    \n",
    "    ## However, we need to cross-validate our classifier. We will do this in a loop and do \n",
    "    # the cross validation step-by-step. Note that in python this wouldn't be necessary. \n",
    "    # You could perform your cross-validation in one line (i.e, you could use \n",
    "    # the cross_val_score function from scikit learn!). That is also possible. \n",
    "    # You could do something like: \n",
    "    # auc = cross_val_score(clf, X, Y, cv=cv, scoring=scorer)\n",
    "    # both ways are fine. I just thought that, personally, it was better for me to learn python \n",
    "    # by doing things step by step, even if I knew there were more simple ways of doing it.\n",
    "    # Why not trying your own way and see what fits you best? :D\n",
    "    # Well, let's go back to our cross-validation:\n",
    "    \n",
    "    \n",
    "    tprs = [] # this variable will take the all the tprs per fold. \n",
    "              # This should give you a variable with n_folds list.\n",
    "        \n",
    "    score = [] # this variable will give you all the scores per fold. \n",
    "               # This will be a vector of size n_folds.\n",
    "    \n",
    "    for train, test in cv.split(X=X, y=Y): # starting our cross-validation. In this for loop, we take \n",
    "                                           # every train and test group (note that python allows you \n",
    "                                           # to have for loops with more than one variable) in the\n",
    "                                           # cv.split. What the method split is doing is just splitting\n",
    "                                           #  our data in the parameters we specified on our cv object. \n",
    "                                           # train and test gives us a list of index.\n",
    "                                           # we use this list to index our X and Y.\n",
    "            # Fit on train set \n",
    "            clf.fit(X[train, :], Y[train])\n",
    "            # Predict on test set\n",
    "            y_pred = clf.predict_proba(X[test, :])\n",
    "            \n",
    "            # use the roc_curve function to get the fpr and tpr of the fold\n",
    "            fold_fpr, fold_tpr, _ = metrics.roc_curve(Y[test], y_pred[:, 1])\n",
    "            # we score the performance of our classifier for this fold\n",
    "            fold_auc = scorer(Y[test], y_pred)\n",
    "            # we then append it in the score and tprs variables. \n",
    "            # To append just means to add a list to a variable \n",
    "            # (to append, really)\n",
    "            score.append(fold)\n",
    "            tprs.append(interp(mean_fpr, fold_fpr, fold_tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "    # Now that we have calculated all the folds we want to know the average across folds. \n",
    "    # We take the average both for the tprs and the score.         \n",
    "    score_tprs.append(np.mean(tprs, 0))\n",
    "    score_conditions.append(np.mean(score))\n",
    "    \n",
    "    # Now we save the cross-validated score and tprs.\n",
    "    np.save(filePath + '\\\\tprs', score_tprs) \n",
    "    np.save(filePath + '\\\\score_conditions', score_conditions) \n",
    "    print('The scores for subject ' + name + 'is ' + str(score_conditions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***OBS***: I am not sure how the function roc_score works. I mainly looked at the documentation online for creating it. I asked more information for the python community and I am waiting for an answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Second part of analysis: decoding whole-trial of directions\n",
    "\n",
    "Now we will use an one-versus-rest strategy to decode directions taking as features the whole trial (n_chan x n_times). The next part of the code is quite similar to the previous. I will try to focus my comments on the things that are different from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for name in subject:    \n",
    "\tfilePath = mainDir + name\n",
    "    \n",
    "    #loading data as epoch object\n",
    "    xDir = filePath + '\\\\struct_cor.mat'\n",
    "    epochs, _ = load_cor(xDir, var_name='struct_cor')\n",
    "    \n",
    "    #Retrieving data as matrix\n",
    "    data = epochs.get_data()\n",
    "    X = np.reshape(data, [data.shape[0], data.shape[1]*data.shape[2]])\n",
    "\n",
    "    # Differently from the last part, the labels for the directions are \n",
    "    # already inside of the epochs object! So we don't need to load anything\n",
    "    # as labels. We just need to retrieve them using the method '.event'. \n",
    "    # The third column of events is the column that contains the information\n",
    "    # we need (if you are curious why, have a look on how MNE define events). \n",
    "    # As it is the third column, the index is number 2 (remember, python \n",
    "    # starts indexation from 0)\n",
    "    directions = epochs.events[:, 2]\n",
    "    \n",
    "    # As we are using an OVR strategy, we need to transform the \n",
    "    # labels '2, 3, 4, 5, 6' into 5 matrix of ones and zeros. The\n",
    "    # object LabelBinarizer will help us with this job. We should \n",
    "    # get in the end a matrix with the number of observations \n",
    "    # (trials) in the first dimension and the number of direction \n",
    "    # in the second one\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    Y = lb.fit_transform(directions) # the method .fit_transform creates \n",
    "                                     # the desired matrix for us.\n",
    "    \n",
    "    \n",
    "    ## Now we will start with the loop over DIRECTIONS. \n",
    "    n_classes = 5                  \n",
    "    directions_scores = []  # this variable will contain in the end \n",
    "                            # the cross-validated AUC \n",
    "                            # values for each direction for the given subject \n",
    "            \n",
    "    for directions in range(n_classes):\n",
    "        y=Y[:, directions] # here we get the set of labels we want. For instance, \n",
    "                           # if we want the labels for the first direction\n",
    "                           # the variable directions will be 0 and we will get \n",
    "                           # the OVR labels for the first direction, and so on.\n",
    "                \n",
    "                \n",
    "        clf = Pipeline([('scaling', StandardScaler()), ('SelectKBest', SelectKBest(f_classif, k=20)), ('svc',  SVC(class_weight='balanced', probability=True, kernel='linear'))])\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "        score = [] # this contains a collection of AUC values for each fold\n",
    "        for train, test in cv.split(X=X, y=y):\n",
    "                # Fit on train set\n",
    "                clf.fit(X[train, :], y[train])\n",
    "                # Predict on test set\n",
    "                y_pred = clf.predict_proba(X[test, :])\n",
    "        \n",
    "                auc_fold = scorer(y[test], y_pred)\n",
    "                score.append(auc_fold)\n",
    "        directions_scores.append(np.mean(score)) # here we append the cross-validated \n",
    "                                                 # score for every direction.\n",
    "                                                 # be careful with indentation!!!\n",
    "    # once everything is done, save the data.        \n",
    "    np.save(filePath + '\\\\score_directions', directions_scores) \n",
    "    print('Whole trial decoding of directions for subject ' + name[0:8] + 'ready')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third part of analysis: time decoding of \"conditions\".\n",
    "\n",
    "After completing the whole trial decoding, it is time to perform the time-decoding for conditions. This means that now we will train one classifier per each time point. This will yield an 1 x n_timepoints vector containing the AUC values for each time point. \n",
    "\n",
    "In this step, we will start retrieving the spatial patterns and filters for each time point. This is the data used to plot the topography I showed in my presentation. For getting the spatial filter and patterns, we use the function LinearModel from MNE,  that was built based on the famous paper. An important observation about how I use this function is mentioned throughout the code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in subject:\n",
    "    \n",
    "    filters = [] # this variable will contain the filters for the given subject\n",
    "    scores = [] # this variable will contain the cross-validated AUC \n",
    "                # over time for the given subject\n",
    "    patterns =[] # this variable will contain the patterns for the given subject\n",
    "       \n",
    "    \n",
    "    filePath = mainDir + name \n",
    "    \n",
    "    # We start the code similarly to part one. \n",
    "    # However, I added this check to verify if I\n",
    "    # had already decoded this subject\n",
    "    # before, to save some time. \n",
    "    \n",
    "    if  os.path.isfile(filePath + '\\\\subject_conditions_time.npy') == False:\n",
    "        \n",
    "        #loading labels for conditions\n",
    "        yDir = filePath + '\\\\trl_conditions.mat'\n",
    "        Y = loadmat(yDir)\n",
    "        conditions = Y['trl_conditions']\n",
    "        Y = conditions.transpose().ravel()\n",
    "        Y[Y==-1] = 0\n",
    "            \n",
    "        \n",
    "        #loading data as epoch object\n",
    "        xDir = mainDir + name + '\\\\struct_cor.mat'\n",
    "        epochs, _ = load_cor(xDir, var_name='struct_cor')\n",
    "        data = epochs.get_data() # data has size: n_trials x n_channels x n_timepoints\n",
    "        \n",
    "        ## Now we are adding one more loop: a loop over time. This is the part that implements the over time decoding.    \n",
    "        n_time = np.size(data, 2)\n",
    "        for point in np.arange(n_time): \n",
    "           \n",
    "            # Here we get the topography from the data in time point 'point'\n",
    "            X = data[:, :, point]\n",
    "            \n",
    "            # Here I create two different types of classifier objects: clf and model. \n",
    "            # Clf will perform a dimensionality reduction, selecting the  most relevant \n",
    "            # channels when doing the decoding. Model, on the other hand, will take \n",
    "            # all the features (all the channels) to retrieve more reliable filters\n",
    "            # and patterns. I did that because keeping all the features when\n",
    "            # decoding made everything significantly slower. In the JR King papers, \n",
    "            # they do NOT perform a dimensionality reduction during the time decoding.\n",
    "            # But this made every subject take about ~5 days in my computer. \n",
    "            # his decision was just I could optimize my time. With a cluster, you can \n",
    "            # play around with the  dimensionality reduction (how, how many features, \n",
    "            # reducing it or keeping all of them). \n",
    "            # This is definitely a point that can be improved.\n",
    "            \n",
    "            clf = Pipeline([('scaling', StandardScaler()), ('SelectKBest', SelectKBest(f_classif, k=20)), ('svc',  SVC(class_weight='balanced', probability=True, kernel='linear'))])       \n",
    "            model = Pipeline([('scaling', StandardScaler()), ('svc',  LinearModel(SVC(class_weight='balanced', kernel='linear')))])\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "            \n",
    "            ## Retrieving the patterns and filters for that time point.\n",
    "            model.fit(X, Y)            \n",
    "            pat = get_coef(model,  'patterns_',  inverse_transform=True) # pat contains the patterns \n",
    "                                                                         # values for time point\n",
    "                \n",
    "            filt = get_coef(model,  'filters_',  inverse_transform=True) # filt contains the filters \n",
    "                                                                         # values for time point\n",
    "            \n",
    "            # we append both pat and filt for this timepoint in patterns\n",
    "            # and filters. Patterns and filters will contain many\n",
    "            # list (n_timepoints lists) containing the patterns \n",
    "            # and filters for the whole epoch.\n",
    "            patterns.append(pat)        \n",
    "            filters.append(filt)\n",
    "            \n",
    "            # Now we perform the decoding for the timepoint. \n",
    "            # This is similar to what we saw before\n",
    "            score = []\n",
    "            for train, test in cv.split(X=X, y=Y):           \n",
    "                \n",
    "                # Fit on train set\n",
    "                clf.fit(X[train, :], Y[train])\n",
    "                # Predict on test set\n",
    "                y_pred = clf.predict_proba(X[test, :])\n",
    "        \n",
    "                fold_auc = scorer(Y[test], y_pred)\n",
    "                score.append(fold_auc)\n",
    "            \n",
    "            scores.append(np.mean(score))\n",
    "                    \n",
    "            print('Subject' + name[0:8] + ', time point: ' + str(point)) \n",
    "            \n",
    "        np.save(filePath + '\\\\subject_conditions_time', scores) \n",
    "        np.save(filePath + '\\\\subject_conditions_patterns', patterns) \n",
    "        np.save(filePath + '\\\\subject_conditions_filters', filters) \n",
    "        print('Time decoding for subject ' + name[0:8] + 'are ready')  \n",
    "    else:\n",
    "        print('Time decoding already ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth part of analysis: time decoding of \"directions\".\n",
    "\n",
    "Now we will use the same OVR strategy over time. I would like you to keep in mind how many classifiers we will train now: every epoch has 611 time points (so 611 classifiers). With the 5-fold cross validation, we will train 5 classifiers per time point - so 5 * 611 = 3055 classifiers. But as we will use a one-versus-rest approach, this means that we will train 3055 classifiers for every 5 directions, so again 3055 * 5 = 15275. So for each subject, we train in total at least 15275 classifiers. If you multiply this number by the number of subjects, *this number will be even higher*. So it is not a surprise that this step is extremly slow, taking many days to finish decoding one subject -- even if you are using the cluster. Here I decided to import the package time to have an idea of how many seconds it takes to decoding on time point.\n",
    "\n",
    "Keeping this in mind, the code is structure in many loops, that are embedded in one another in this order: the first loop is across subjects, the second loop is across directions, the third loop is across time points, and the fourth loop is the cross validation. By now, the general structure of the workflow should be familiar for you, so comments will be restricted to the new features of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "for name in subject: \n",
    "        \n",
    "    print('SVM for subject ' + name)\n",
    "    filePath = mainDir + name\n",
    "    \n",
    "    #loading data as epoch object\n",
    "    print('Loading data')\n",
    "    xDir = filePath + '\\\\struct_cor.mat'\n",
    "    epochs, _ = load_cor(xDir, var_name='struct_cor')\n",
    "    \n",
    "    #Retrieving data as matrix\n",
    "    data = epochs.get_data()\n",
    "    \n",
    "    #Retrieving labels for directions - remember, this is a similar step to what was done before (part 2)\n",
    "    directions = epochs.events[:, 2]\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    Y = lb.fit_transform(directions)\n",
    "    \n",
    "    n_classes= np.size(Y, 2)\n",
    "    directions_time_scores =[] # will contain collections of n_directions \n",
    "                               # lists containing the scores over time.\n",
    "        \n",
    "    directions_patterns=[]     # will  collections of n_directions lists \n",
    "                               #containing the patterns over time.\n",
    "        \n",
    "    directions_filters=[]      # will  collections of n_directions \n",
    "                               # lists containing the filters over time.\n",
    "    for directions in range(n_classes):\n",
    "        print('directions ' + str(directions))\n",
    "        y=Y[:, directions]\n",
    "        \n",
    "        print('defining models')\n",
    "        clf = Pipeline([('scaling', StandardScaler()), ('SelectKBest', SelectKBest(f_classif, k=20)), ('svc',  SVC(class_weight='balanced', probability=True, kernel='linear'))])             \n",
    "        model =  Pipeline([('scaling', StandardScaler()), ('svc',  LinearModel(SVC(class_weight='balanced', probability=True, kernel='linear')))])             \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        print('models defined')\n",
    "        \n",
    "    \n",
    "        n_time = np.size(data, 2)             \n",
    "        scores = [] # vector of scores over time for a given direction\n",
    "        patterns = []  # group of lists of patterns over time for a given direction\n",
    "        filters = [] # group of lists of filters over time for a given direction\n",
    "        for point in np.arange(n_time):\n",
    "            t0 = time.time() # this is used by package time to count time. similar to tic toc in matlab\n",
    "            print('computing SVMs for time point ' + str(point))\n",
    "            X = data[:, :, point]\n",
    "            \n",
    "            print('fitting patterns')\n",
    "            model.fit(X, y)            \n",
    "            pat = get_coef(model,  'patterns_',  inverse_transform=True)\n",
    "            filt = get_coef(model,  'filters_',  inverse_transform=True)\n",
    "            print('patterns ready')\n",
    "            patterns.append(pat)\n",
    "            filters.append(filt) \n",
    "            \n",
    "            score = [] # group of scores for each fold for a given time point\n",
    "            n = 0\n",
    "            for train, test in cv.split(X=X, y=y):\n",
    "                n = n+1\n",
    "                print('cv fold number ' + str(n))\n",
    "                # Fit on train set\n",
    "                clf.fit(X[train, :], y[train])\n",
    "                # Predict on test set\n",
    "                y_pred = clf.predict_proba(X[test, :])\n",
    "                fold_auc = scorer(y[test], y_pred)\n",
    "                score.append(fold_auc)\n",
    "            t1 = time.time()\n",
    "            print('taking the mean of the fold')\n",
    "            scores.append(np.mean(score)) # cross validated AUC for time point\n",
    "            \n",
    "            total = (t1-t0) # this is for counting the time\n",
    "            print('Subject' + name[0:8] + ', time point: ' + str(point) + 'total time = ', str(total))\n",
    "        directions_time_scores.append(scores) #  collections of n_directions lists \n",
    "                                              # containing the scores over time.\n",
    "            \n",
    "        directions_patterns.append(patterns)  #  collections of n_directions lists \n",
    "                                              # containing the patterns over time.\n",
    "            \n",
    "        directions_filters.append(filters)    #  collections of n_directions lists \n",
    "                                              # containing the filters over time.   \n",
    "    \n",
    "    np.save(filePath + '\\\\subject_directions_time', directions_time_scores) \n",
    "    np.save(filePath + '\\\\subject_directions_patterns', directions_patterns)\n",
    "    np.save(filePath + '\\\\subject_directions_filters', directions_filters)  \n",
    "    print('Time decoding of direction for subject ' + name[0:8] + 'are ready')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Extra part\n",
    "\n",
    "**Why an \"extra\" part?**\n",
    "\n",
    "During my rotation, I've tried many things - some of them would work, some of them not. I decided to say that everything that used an SVM should be the \"main part\" -- because I thought that I would be able to compare with all the other papers I had read about decoding of EEG data (in which they use mainly SVMs). However, at some point, I decided that I could also try other *classifiers*. I call this part of the script in which I try different models as \"extra\". \n",
    "\n",
    "I first tried to use [support vector regression](http://scikit-learn.org/stable/modules/kernel_ridge.html) to do an ordinal regression on the data. As every loudspeaker is ordered, this could be a way of reducing the number of classifiers I had to train in the direction problem and thus give a quicker answer I could show in the presentation. I inspired in the method used by JR King for ordinal regression. That is what I implement in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole trial decoding of directions using SVRs.\n",
    "\n",
    "Repeating second part of analysis (decoding directions) using linear SVR. The result will be a single value for each subject.  \n",
    "\n",
    "In this part, we are setting a new scorer: The measure performance here is the non-parametric rho correlation value. We also import the LinearSVR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding subject AB171109_20171109_111227v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.019599838302001012])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.019599838302001012, 0.24205800302971248])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.019599838302001012, 0.24205800302971248, 0.29595755836021526])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.019599838302001012, 0.24205800302971248, 0.29595755836021526, 0.22245816472771146])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.019599838302001012, 0.24205800302971248, 0.29595755836021526, 0.22245816472771146, 0.28811762303941485])\n",
      "saving...\n",
      "Final score for subjectAB171109_20171109_111227v03HTis: 0.205798302171\n",
      "decoding subject AB171219_20171219_045837v02CI\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting...\n",
      "('score:', [-0.74586985398289152])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.74586985398289152, -0.09323373174786144])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.74586985398289152, -0.09323373174786144, -0.29665278283410457])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.74586985398289152, -0.09323373174786144, -0.29665278283410457, -0.67283849489430991])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.74586985398289152, -0.09323373174786144, -0.29665278283410457, -0.67283849489430991, 0.87208159927238094])\n",
      "saving...\n",
      "Final score for subjectAB171219_20171219_045837v02CIis: -0.187302652837\n",
      "decoding subject AB171219_20171219_045837v03CI\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.12164566778234287])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.12164566778234287, 0.35538757783504582])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.12164566778234287, 0.35538757783504582, 0.076946782101937172])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.12164566778234287, 0.35538757783504582, 0.076946782101937172, -0.12507725251737709])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.12164566778234287, 0.35538757783504582, 0.076946782101937172, -0.12507725251737709, -0.14070114896147978])\n",
      "saving...\n",
      "Final score for subjectAB171219_20171219_045837v03CIis: 0.0576403252481\n",
      "decoding subject AG180117_20180117_011420v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.19893835876531024])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.19893835876531024, 0.14209882768950732])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.19893835876531024, 0.14209882768950732, -0.064679466396603325])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.19893835876531024, 0.14209882768950732, -0.064679466396603325, 0.1107390864063057])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.19893835876531024, 0.14209882768950732, -0.064679466396603325, 0.1107390864063057, 0.18025578780762833])\n",
      "saving...\n",
      "Final score for subjectAG180117_20180117_011420v03HTis: 0.0338951753483\n",
      "decoding subject AH171123_20171123_051919v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.18972168466360728])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.18972168466360728, 0.081368987333574144])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.18972168466360728, 0.081368987333574144, 0.39339904092178724])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.18972168466360728, 0.081368987333574144, 0.39339904092178724, 0.30995730783561626])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.18972168466360728, 0.081368987333574144, 0.39339904092178724, 0.30995730783561626, 0.0099404229791238367])\n",
      "saving...\n",
      "Final score for subjectAH171123_20171123_051919v03HTis: 0.196877488747\n",
      "decoding subject AR171127_20171127_011246v02HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.39532663098631837])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.39532663098631837, 0.23797094426904664])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.39532663098631837, 0.23797094426904664, 0.36142955445373998])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.39532663098631837, 0.23797094426904664, 0.36142955445373998, 0.1750540695395997])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.39532663098631837, 0.23797094426904664, 0.36142955445373998, 0.1750540695395997, -0.24301391027775829])\n",
      "saving...\n",
      "Final score for subjectAR171127_20171127_011246v02HTis: 0.185353457794\n",
      "decoding subject AR171127_20171127_011246v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316, 0.010779911066100555])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316, 0.010779911066100555, 0.18418236540335597])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316, 0.010779911066100555, 0.18418236540335597, 0.35495957819690377])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316, 0.010779911066100555, 0.18418236540335597, 0.35495957819690377, 0.11615264138250042])\n",
      "saving...\n",
      "Final score for subjectAR171127_20171127_011246v03HTis: 0.124198973591\n",
      "decoding subject AS171221_20171221_061919v02CI\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.80728273399090023])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.80728273399090023, 0.81775057076113478])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.80728273399090023, 0.81775057076113478, 0.8827682051150797])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.80728273399090023, 0.81775057076113478, 0.8827682051150797, 0.89812069563882024])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.80728273399090023, 0.81775057076113478, 0.8827682051150797, 0.89812069563882024, 0.87432433532702258])\n",
      "saving...\n",
      "Final score for subjectAS171221_20171221_061919v02CIis: 0.856049308167\n",
      "decoding subject DB171120_20171120_041048v02HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.27376237442445589])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.27376237442445589, 0.077664800875166015])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.27376237442445589, 0.077664800875166015, 0.0086677385663407342])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.27376237442445589, 0.077664800875166015, 0.0086677385663407342, 0.0059098217497777729])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.27376237442445589, 0.077664800875166015, 0.0086677385663407342, 0.0059098217497777729, -0.18196414290977025])\n",
      "saving...\n",
      "Final score for subjectDB171120_20171120_041048v02HTis: 0.0368081185412\n",
      "decoding subject DB171120v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29497756644511519])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29497756644511519, -0.13523888428380698])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29497756644511519, -0.13523888428380698, 0.3802368630588196])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29497756644511519, -0.13523888428380698, 0.3802368630588196, 0.27439773622801417])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29497756644511519, -0.13523888428380698, 0.3802368630588196, 0.27439773622801417, 0.0072893732793379452])\n",
      "saving...\n",
      "Final score for subjectDB171120v03HTis: 0.164332530945\n",
      "decoding subject DD171115_20171115_051507v02HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.26164592394755959])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.26164592394755959, 0.63389995313073699])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.26164592394755959, 0.63389995313073699, -0.23260317860884255])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.26164592394755959, 0.63389995313073699, -0.23260317860884255, 0.17087387351649588])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.26164592394755959, 0.63389995313073699, -0.23260317860884255, 0.17087387351649588, 0.068886325972618762])\n",
      "saving...\n",
      "Final score for subjectDD171115_20171115_051507v02HTis: 0.0758822100127\n",
      "decoding subject DD171115_20171115_051507v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.017639854471800911])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.017639854471800911, 0.41453658008732136])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.017639854471800911, 0.41453658008732136, 0.3214373481528166])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.017639854471800911, 0.41453658008732136, 0.3214373481528166, 0.12739894896300655])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.017639854471800911, 0.41453658008732136, 0.3214373481528166, 0.12739894896300655, 0.26835307058362695])\n",
      "saving...\n",
      "Final score for subjectDD171115_20171115_051507v03HTis: 0.229873160452\n",
      "decoding subject DF180109_20180109_105424v02HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.13536736638348043])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.13536736638348043, 0.5568425024118957])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.13536736638348043, 0.5568425024118957, 0.034061113367274964])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.13536736638348043, 0.5568425024118957, 0.034061113367274964, 0.14616334912000961])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.13536736638348043, 0.5568425024118957, 0.034061113367274964, 0.14616334912000961, 0.038654352278582257])\n",
      "saving...\n",
      "Final score for subjectDF180109_20180109_105424v02HTis: 0.128070790159\n",
      "decoding subject DF180109_20180109_105424v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.13425889236870692])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.13425889236870692, 0.47725606265372461])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.13425889236870692, 0.47725606265372461, -0.0042694900636122248])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.13425889236870692, 0.47725606265372461, -0.0042694900636122248, 0.12590872678551221])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.13425889236870692, 0.47725606265372461, -0.0042694900636122248, 0.12590872678551221, 0.027828236997890731])\n",
      "saving...\n",
      "Final score for subjectDF180109_20180109_105424v03HTis: 0.152196485748\n",
      "decoding subject DG171122_20171122_033719v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.40767663668162102])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.40767663668162102, 0.24205800302971248])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.40767663668162102, 0.24205800302971248, 0.38905679029472007])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.40767663668162102, 0.24205800302971248, 0.38905679029472007, 0.31261742091691608])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.40767663668162102, 0.24205800302971248, 0.38905679029472007, 0.31261742091691608, 0.00044314554082010456])\n",
      "saving...\n",
      "Final score for subjectDG171122_20171122_033719v03HTis: 0.270370399293\n",
      "decoding subject GE180115_20180115_040922v03CI\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.078399353208004049])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.078399353208004049, 0.15385873067070793])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.078399353208004049, 0.15385873067070793, 0.38807679837962006])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.078399353208004049, 0.15385873067070793, 0.38807679837962006, 0.55864034739634427])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.078399353208004049, 0.15385873067070793, 0.38807679837962006, 0.55864034739634427, 0.33030907393148562])\n",
      "saving...\n",
      "Final score for subjectGE180115_20180115_040922v03CIis: 0.301856860717\n",
      "decoding subject JA171123_20171123_105103v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.26079115077263154])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.26079115077263154, 0.018055772388740891])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.26079115077263154, 0.018055772388740891, 0.2843319867175792])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.26079115077263154, 0.018055772388740891, 0.2843319867175792, 0.22141597263539145])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.26079115077263154, 0.018055772388740891, 0.2843319867175792, 0.22141597263539145, 0.11252287133929731])\n",
      "saving...\n",
      "Final score for subjectJA171123_20171123_105103v03HTis: 0.179423550771\n",
      "decoding subject JH180105_20180105_104727v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.011317526586655397])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.011317526586655397, 0.12866595789674495])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.011317526586655397, 0.12866595789674495, 0.2985477324577574])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.011317526586655397, 0.12866595789674495, 0.2985477324577574, -0.043025802101706798])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.011317526586655397, 0.12866595789674495, 0.2985477324577574, -0.043025802101706798, -0.0082633628813729987])\n",
      "saving...\n",
      "Final score for subjectJH180105_20180105_104727v03HTis: 0.0774484103916\n",
      "decoding subject LW171115_20171115_113123v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316, 0.095059215764704899])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316, 0.095059215764704899, 0.33809721070951743])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316, 0.095059215764704899, 0.33809721070951743, -0.02057983021710106])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.045079628094602316, 0.095059215764704899, 0.33809721070951743, -0.02057983021710106, -0.24597797069011271])\n",
      "saving...\n",
      "Final score for subjectLW171115_20171115_113123v03HTis: 0.0243037994945\n",
      "decoding subject MU180122_20180122_035931v03CI\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.047039611924802424])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.047039611924802424, -0.038219684688901974])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.047039611924802424, -0.038219684688901974, 0.0019599838302001007])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.047039611924802424, -0.038219684688901974, 0.0019599838302001007, 0.19020057563872511])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.047039611924802424, -0.038219684688901974, 0.0019599838302001007, 0.19020057563872511, 0.20101669533390104])\n",
      "saving...\n",
      "Final score for subjectMU180122_20180122_035931v03CIis: 0.0803994364077\n",
      "decoding subject RA171121_20171121_041254v02HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.046096609648266632])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.046096609648266632, 0.23214144527910133])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.046096609648266632, 0.23214144527910133, 0.0024944681829856419])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.046096609648266632, 0.23214144527910133, 0.0024944681829856419, -0.12066989835193043])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.046096609648266632, 0.23214144527910133, 0.0024944681829856419, -0.12066989835193043, 0.062075865060127806])\n",
      "saving...\n",
      "Final score for subjectRA171121_20171121_041254v02HTis: 0.0259890541044\n",
      "decoding subject RA171121_20171121_041254v03HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.43315642647422231])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.43315642647422231, 0.041159660434202119])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.43315642647422231, 0.041159660434202119, -0.0019599838302001007])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.43315642647422231, 0.041159660434202119, -0.0019599838302001007, 0.055659571682944733])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.43315642647422231, 0.041159660434202119, -0.0019599838302001007, 0.055659571682944733, 0.18911356374481886])\n",
      "saving...\n",
      "Final score for subjectRA171121_20171121_041254v03HTis: 0.143425847701\n",
      "decoding subject RB180116_20180116_112722v03CI\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.60269502778653106])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.60269502778653106, 0.37337691965311931])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.60269502778653106, 0.37337691965311931, 0.25325365450499832])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.60269502778653106, 0.37337691965311931, 0.25325365450499832, 0.28251973267088687])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.60269502778653106, 0.37337691965311931, 0.25325365450499832, 0.28251973267088687, 0.29674192814631922])\n",
      "saving...\n",
      "Final score for subjectRB180116_20180116_112722v03CIis: 0.361717452552\n",
      "decoding subject TV171120_20171120_125640v02HT\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.47043028884478022])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.47043028884478022, 0.10665900855453468])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.47043028884478022, 0.10665900855453468, -0.033436233104231321])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.47043028884478022, 0.10665900855453468, -0.033436233104231321, 0.35529903775243932])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.47043028884478022, 0.10665900855453468, -0.033436233104231321, 0.35529903775243932, -0.090934054517115059])\n",
      "saving...\n",
      "Final score for subjectTV171120_20171120_125640v02HTis: 0.161603609506\n",
      "decoding subject WE171214_20171214_053845v02CI\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29665278283410457])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29665278283410457, -0.025427381385780389])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29665278283410457, -0.025427381385780389, 0.54245080289664827])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29665278283410457, -0.025427381385780389, 0.54245080289664827, 0.71526975132908632])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.29665278283410457, -0.025427381385780389, 0.54245080289664827, 0.71526975132908632, -0.051298917604257713])\n",
      "saving...\n",
      "Final score for subjectWE171214_20171214_053845v02CIis: 0.295529407614\n",
      "decoding subject WE171214_20171214_053845v03CI\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.88003273975984542])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.88003273975984542, 0.79183346740084082])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.88003273975984542, 0.79183346740084082, 0.70657417078713636])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.88003273975984542, 0.79183346740084082, 0.70657417078713636, 0.47932836006846519])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [0.88003273975984542, 0.79183346740084082, 0.70657417078713636, 0.47932836006846519, 0.49889146647208316])\n",
      "saving...\n",
      "Final score for subjectWE171214_20171214_053845v03CIis: 0.671332040898\n",
      "decoding subject melon180125_20180125_112349v03ME\n",
      "loading epochs\n",
      "reshaping data\n",
      "defining estimator\n",
      "creating cross val\n",
      "('computing fold: ', 0)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.0029399757453001515])\n",
      "('computing fold: ', 1)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.0029399757453001515, 0.081339328953304188])\n",
      "('computing fold: ', 2)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.0029399757453001515, 0.081339328953304188, 0.13033892470830674])\n",
      "('computing fold: ', 3)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.0029399757453001515, 0.081339328953304188, 0.13033892470830674, 0.062719482566403223])\n",
      "('computing fold: ', 4)\n",
      "fitting...\n",
      "predicting...\n",
      "('score:', [-0.0029399757453001515, 0.081339328953304188, 0.13033892470830674, 0.062719482566403223, 0.36316699016701554])\n",
      "saving...\n",
      "Final score for subjectmelon180125_20180125_112349v03MEis: 0.12692495013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Scorer: non parametric R²\n",
    "def scorer(y_true, y_pred):\n",
    "    from scipy.stats import spearmanr\n",
    "    rho, p = spearmanr(y_true, y_pred)\n",
    "    return rho\n",
    "\n",
    "\n",
    "for name in subject:\n",
    "    directions_svr_scores = []\n",
    "    print('decoding subject ' + name)\n",
    "    filePath = mainDir + name\n",
    "   \n",
    "    #loading data as epoch object\n",
    "    xDir = filePath + '\\\\struct_cor.mat'\n",
    "    print('loading epochs')\n",
    "    epochs, _ = load_cor(xDir, var_name='struct_cor')\n",
    "    \n",
    "    #Retrieving data as matrix\n",
    "    data = epochs.get_data()\n",
    "    print('reshaping data')\n",
    "    X = np.reshape(data, [data.shape[0], data.shape[1]*data.shape[2]])\n",
    "\n",
    "    y = epochs.events[:, 2]  \n",
    "    \n",
    "    \n",
    "    # in order to make results more interpretable, we remove the trials in which the \n",
    "    # last sound was different.\n",
    "    yDir = mainDir + name + '\\\\trl_conditions.mat'\n",
    "    Y = loadmat(yDir)\n",
    "    conditions = Y['trl_conditions']\n",
    "    Y = conditions.transpose().ravel()\n",
    "    \n",
    "    X = X[Y==-1, :]\n",
    "    y = y[Y==-1]\n",
    "\n",
    "    ## From JR King python notebook\n",
    "    # Estimator\n",
    "    print('defining estimator')\n",
    "    clf = Pipeline([('scaling', StandardScaler()), ('anova', SelectKBest(f_classif, k=20)),  ('svr', LinearSVR())])\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    y_test =[]\n",
    "    r =[]\n",
    "    # Fit, predict, and score\n",
    "    print('creating cross val')\n",
    "    i = -1\n",
    "    for train, test in cv.split(X=X, y=y):\n",
    "        i = i + 1\n",
    "        print('computing fold: ', i)\n",
    "        print('fitting...')\n",
    "        clf.fit(X[train, :], y[train])\n",
    "        print('predicting...')\n",
    "        y_pred = clf.predict(X[test, :])\n",
    "        r.append(scorer(y[test], y_pred))  # score in [-1, 1], chance = 0.\n",
    "        print('score:', r)  # should be > 0.\n",
    "    \n",
    "    directions_svr_scores.append(np.mean(r))\n",
    "    print('saving...')\n",
    "    np.save(filePath + '\\\\directions_svr_scores', directions_svr_scores) \n",
    "    print('Final score for subject' + name + 'is: '+  str(np.mean(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: using SVR scores with a pearson regression to decode directions. \n",
    "\n",
    "This eans each subject is going to have one temporal decoding time. Assuming that there is information in the data about direction, the r value would peak in the time point information is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in subject:\n",
    "    directions_svr_time_scores =[]\n",
    "    print('decoding subject ' + name)\n",
    "    filePath = mainDir + name\n",
    "   \n",
    "    #loading data as epoch object\n",
    "    xDir = filePath + '\\\\struct_cor.mat'\n",
    "    print('loading epochs')\n",
    "    epochs, _ = load_cor(xDir, var_name='struct_cor')\n",
    "    \n",
    "    #Retrieving data as matrix\n",
    "    data = epochs.get_data()\n",
    "    print('reshaping data')\n",
    "\n",
    "    y = epochs.events[:, 2]  \n",
    "    \n",
    "    # in order to make results more interpretable, we remove the trials in which the \n",
    "    # last sound was different.\n",
    "    print('removing ~different~ conditions')\n",
    "    yDir = filePath + '\\\\trl_conditions.mat'\n",
    "    Y = loadmat(yDir)\n",
    "    conditions = Y['trl_conditions']\n",
    "    Y = conditions.transpose().ravel()\n",
    "    \n",
    "    data = data[Y==-1, :, :]\n",
    "    y = y[Y==-1]\n",
    "        \n",
    "    \n",
    "    scores = []\n",
    "    n_time = np.size(data, 2)\n",
    "    i = -1\n",
    "    for point in np.arange(n_time):\n",
    "        i = i + 1\n",
    "        print('time point ' + str(i))\n",
    "        X = data[:, :, point]\n",
    "        ## From JR King python notebook\n",
    "        # Estimator\n",
    "        print('defining estimator')\n",
    "        clf = Pipeline([('scaling', StandardScaler()), ('anova', SelectKBest(f_classif, k=20)),  ('svr', LinearSVR())])\n",
    "        cv = StratifiedKFold(n_splits=5)\n",
    "        y_test =[]\n",
    "        r =[]\n",
    "        # Fit, predict, and score\n",
    "        print('creating cross val')\n",
    "        q = -1\n",
    "        r = []\n",
    "        for train, test in cv.split(X=X, y=y):\n",
    "            q = q + 1\n",
    "            print('computing fold: ', q)\n",
    "            print('fitting...')\n",
    "            clf.fit(X[train, :], y[train])\n",
    "            print('predicting...')\n",
    "            y_pred = clf.predict(X[test, :])\n",
    "            r.append(scorer(y[test], y_pred))  # score in [-1, 1], chance = 0.\n",
    "            print('score:', r)  # should be > 0.\n",
    "        scores.append(np.mean(r)) \n",
    "    directions_svr_time_scores.append(scores)\n",
    "    print('saving...')\n",
    "    np.save(filePath + '\\\\directions_svr_time_scores', directions_svr_time_scores) \n",
    "    print('Final score for subject' + name + 'is: '+  str(np.mean(r)))  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
