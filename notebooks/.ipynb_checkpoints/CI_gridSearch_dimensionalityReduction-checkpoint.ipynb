{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code used for Decoding\n",
    "### Documentation created by Ana Vedoveli in 27 Feb 2017\n",
    "\n",
    "\n",
    "This is a code used for parameter optimization on decoding tasks - mainly to refine feature selection procedures. When I worte this, I was beginning learning python, so anyone with a keen eye will perceive that major parts of this code are similar to the scikit-learn tutorial. However, having this tutorial still might be helpful to others.\n",
    "\n",
    "I mainly used the GridSearch CV to find the best feature reduction technique and the best number of features to keep. I also only did this using **whole trial decoding**. My reasoning here was that whatever parameters I found on the whole trial decoding could be generalized to the time-decoding. Otherwise, I would have to use GridSearch CV on *every* n_times classifiers trained during the time decoding.\n",
    "\n",
    "**OBS**: This tutorial needs a reasonable understanding on how dictionaires work on python.\n",
    "\n",
    "**OBS2**: Sometimes people use nested gridsearch methods (a grid search inside of a different crossvalidation fold) as they assume that the non-nested version might give inflated results. This tutorial implements a NON-Nested gridsearch CV because of the computational nested gridsearch imply.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start importing the important functions we are using in this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import (loadmat, savemat)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from mne.datasets import sample\n",
    "from sklearn.preprocessing import LabelEncoder            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define our own functions. There are three functions in this code: the load_cor function -- to load the data from matlab, the grid_dim_red that implements my routine for comparing the performance of the parameters using GridSearchCV, and function plot_dimcom that plots the results of the comparisons (*NOTE that in this example I am computing the AUC on the CATEGORICAL OUTPUTS of the SVM, and not in the probabilistic*). We will go through each function now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This code is meant to work on WINDOWS. \n",
    "def load_cor(xDir, var_name='struct_cor'):\n",
    "    import mne\n",
    "    from mne import create_info\n",
    "    from mne.epochs import EpochsArray\n",
    "    import scipy.io as sio\n",
    "    import numpy as np\n",
    "    # load Matlab/Fieldtrip data\n",
    "    mat = sio.loadmat(xDir, squeeze_me=True, struct_as_record=False)\n",
    "    ft_data = mat[var_name]\n",
    "    event = ft_data.trialinfo[:, 1]\n",
    "\n",
    "    # convert to mne\n",
    "    n_trial, n_chans, n_time = ft_data.trial.shape\n",
    "    data = np.zeros((n_trial, n_chans, n_time))\n",
    "    data = ft_data.trial\n",
    "\n",
    "    sfreq = 200\n",
    "    time = ft_data.time\n",
    "\n",
    "    \n",
    "    coi = range(n_chans)\n",
    "    data = data[:, coi, :]\n",
    "    chan_names = [l.encode('ascii') for l in ft_data.label[coi]]\n",
    "    chan_types = ft_data.label[coi]\n",
    "    chan_types[:] = 'eeg'\n",
    "    info = create_info(chan_names, sfreq, chan_types)\n",
    "    events = np.array([np.arange(n_trial), np.zeros(n_trial), event], int).T\n",
    "    epochs = EpochsArray(data, info, events=events,\n",
    "                         tmin=np.min(time), verbose=False)\n",
    "    montage = mne.channels.read_montage('GSN-HydroCel-257')\n",
    "    epochs.set_montage(montage)\n",
    "    return epochs, ft_data.trialinfo\n",
    "    \n",
    "\n",
    "## This function is the proper gridsearch function.\n",
    "# mainPath = path of your CI\\Python\\Subjects folder\n",
    "# name = name of your subject\n",
    "# varname = name of the type of data. Can be _cor or _inter\n",
    "# N_features = number of features to be tested\n",
    "# c_options = values of C to be tested\n",
    "def grid_dim_red(mainPath, name, varname, N_FEATURES_OPTIONS, C_OPTIONS):\n",
    "\n",
    "    print('working on subject ' + name)\n",
    "    \n",
    "    # This is the same as the decoding script.\n",
    "    # If you haven't seen the tutorial for the decoding\n",
    "    # script yet, I would suggest that you should\n",
    "    # read that tutorial first.\n",
    "    filePath = mainPath + name + '\\\\'\n",
    "    #loading labels for conditions\n",
    "    yDir = filePath + 'trl_conditions.mat'\n",
    "    Y = loadmat(yDir)\n",
    "    conditions = Y['trl_conditions']\n",
    "    Y = conditions.transpose().ravel()\n",
    "    Y[Y==-1] = 0\n",
    "    \n",
    "    #loading data as epoch object\n",
    "    print('loading data...')\n",
    "    xDir = filePath + varname + '.mat'\n",
    "    epochs, _ = load_cor(xDir, var_name=varname)\n",
    "    \n",
    "    #Retrieving data as matrix\n",
    "    data = epochs.get_data()\n",
    "    # As we are using this function to find the best\n",
    "    # parameters for the whole trial decoding, we \n",
    "    # collapes the dimensions of channels and time points\n",
    "    X = np.reshape(data, [data.shape[0], data.shape[1]*data.shape[2]])\n",
    "    \n",
    "    \n",
    "    ## Now we start to prepare for the gridsearch CV.\n",
    "    # The gridsearch CV takes as an input a dictionary \n",
    "    # with lists of parameters you would like to test.\n",
    "    # From the python documentation, the param_grid should be:\n",
    "    # \"A Dictionary with parameters names (string) \n",
    "    # as keys and lists of parameter settings to try as values, or a list of such \n",
    "    # dictionaries, in which case the grids spanned by each dictionary \n",
    "    # in the list are explored. \n",
    "    # This enables searching over any sequence of parameter settings.\"\n",
    "    # We will go through this definition with more details in\n",
    "    # the next lines.\n",
    "    \n",
    "    \n",
    "    # Here we test three different Dim_red techniques:\n",
    "    # - PCA\n",
    "    # - Univariate feature reduction\n",
    "    # - KClustering\n",
    "    print('defining reduction techniques')\n",
    "    param_grid = [ # Here we define a *list of dictionaires*.\n",
    "                   # List as defined with [].\n",
    "                   # We have in total *3 dictionaires* with\n",
    "                   # the different reduction methods we want \n",
    "                   # to try.\n",
    "        { # This is our first dictionary. Dictionaires\n",
    "          # are defined with {}. It is defining parame-\n",
    "          # ters to be tested for the PCA.\n",
    "            'reduce_dim': [PCA(iterated_power=7)], # this is our first dictionaire entry.\n",
    "                                                   # the key name is 'reduce_dim', and it\n",
    "                                                   # equal to the reduction technique you\n",
    "                                                   # want to try.\n",
    "            'reduce_dim__n_components': N_FEATURES_OPTIONS, # this is our first dictionaire\n",
    "                                                   # entry. the key name is 'reduce_dim\n",
    "                                                   # __n_components' and it is equal to\n",
    "                                                   # a list of the number of COMPONENTS\n",
    "                                                   # of the PCA you would like to keep.\n",
    "            \n",
    "            # Pay attention that the name of the keys are NOT RANDOM.\n",
    "            # They have to do with the function attributes.\n",
    "            # For instance, the PCA function takes an input\n",
    "            # named \"n_components\", that is the number of \n",
    "            # components you want to keep. Similarly, in the\n",
    "            # next entry of our dictionary, the classifier\n",
    "            # (named by classify -- see the comments next to the\n",
    "            # pipeline some lines below) takes the input C\n",
    "            # thus the name of the key is classify__C. \n",
    "            \n",
    "            'classify__C': C_OPTIONS # this is our first dictionaire\n",
    "                                     # entry. the key name is 'classify\n",
    "                                     # __C' and it is equal to\n",
    "                                     # a list of the number of C values\n",
    "                                     # you would like to try out.\n",
    "        },  \n",
    "        \n",
    "        # This is the second dictionary. It is defining parame-\n",
    "        # ters to be tested for the Univariate feature selection.\n",
    "        # Similar to the previous example.\n",
    "        {\n",
    "            'reduce_dim': [SelectKBest(f_classif)],\n",
    "            'reduce_dim__k': N_FEATURES_OPTIONS, # here the name is \n",
    "                                                 # reduce_dim__k because\n",
    "                                                 # f_classif takes k as an\n",
    "                                                 # input. K is the number\n",
    "                                                 # features you want to\n",
    "                                                 # keep. \n",
    "            'classify__C': C_OPTIONS\n",
    "        },  \n",
    "        # This is the third dictionary. It is defining parame-\n",
    "        # ters to be tested for the K-cluster feature reduction.\n",
    "        # Similar to the previous example.\n",
    "        {\n",
    "            'reduce_dim': [MiniBatchKMeans()],\n",
    "            'reduce_dim__n_clusters': N_FEATURES_OPTIONS, # here the name is \n",
    "                                                 # reduce_dim__n_clusters \n",
    "                                                 # because the cluster function\n",
    "                                                 # KMeans takes n_clusters as an\n",
    "                                                 # input. \n",
    "            'classify__C': C_OPTIONS\n",
    "        },  \n",
    "        \n",
    "        ## REMEMBER: C_OPTIONS AND N_FEATURES OPTIONS ARE INPUT OF OUR\n",
    "        ## FUNCTION GRID_DIM_RED\n",
    "        \n",
    "    ]\n",
    "    # now we define the labels for our reduction techniques, in the same\n",
    "    # order as they appear in the list of dictionaires.\n",
    "    reducer_labels = ['PCA', 'KBest(f_classif)', 'Clustering (K-means)']\n",
    "    \n",
    "    ## PIPELINE:\n",
    "    # This is the pipeline for your classifier. \n",
    "    # The pipeline takes an important role in this\n",
    "    # code.\n",
    "    pipe = Pipeline([ # we give a name for each step.\n",
    "                      # So the StandardScaler step is\n",
    "                      # called 'scalling', the classifier\n",
    "                      # is called \"classify. The 'reduce_dim' \n",
    "                      # field will change according to\n",
    "                      # what you have previously specified \n",
    "                      # in param_grid\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('reduce_dim', SelectKBest(f_classif)), # here we do not define K,\n",
    "                                                # this was defined in the \n",
    "                                                # param_grid dictionairies.\n",
    "        ('classify',  SVC(class_weight='balanced', probability=False, kernel='linear'))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Defining cv folds parameter\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    print('training gridsearch')\n",
    "    # Now we actually perform the gridsearch. we actually use \n",
    "    # the gridsearch cv function.\n",
    "    grid = GridSearchCV(pipe, cv=inner_cv, n_jobs=1, param_grid=param_grid, scoring='roc_auc')\n",
    "    # now we fit our grid object with the data. This step takes a\n",
    "    # reaaally long time.\n",
    "    grid.fit(X, Y)\n",
    "    \n",
    "    print('saving results')\n",
    "    # Now we want to get the comparison of the grid search.\n",
    "    # The object grid has a very important attribute:\n",
    "    # 'cv_results_'. This attribute is a dictionairie\n",
    "    # with different statistics of our gridsearch, as\n",
    "    # for instance, the mean time across folds for each\n",
    "    # tested model, the mean and std results across\n",
    "    # folds of each model. Here we want the key with\n",
    "    # the mean results, and the name of the key is\n",
    "    # 'mean_test_score'. In the next lines we get\n",
    "    # the values inside of this key and save it.\n",
    "    reduction_results = np.array(grid.cv_results_['mean_test_score'])\n",
    "    np.save(filePath + 'reduction_results', reduction_results) \n",
    "    return grid, reduction_results # the function in the end\n",
    "                                   # returns the grid object \n",
    "                                   # and the reduction results.\n",
    "\n",
    "\n",
    "## We now computed the gridsearch, but we to visualize it.\n",
    "# So this is what our next function will do!\n",
    "        \n",
    "# This functions plots the results of the gridsearch\n",
    "def plot_dimcomp(name, mean_scores=reduction_results):\n",
    "    \n",
    "    # scores are in the order of param_grid iteration, which is alphabetical\n",
    "    mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "    # select score for best C \n",
    "    # (in the case you have tried \n",
    "    #  more than one C)\n",
    "    mean_scores = mean_scores.max(axis=0)\n",
    "    \n",
    "    # Now we just plot. Feel free to change.\n",
    "    # This plotting was completely taken\n",
    "    # from the scikit website!\n",
    "    bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n",
    "                   (len(reducer_labels) + 1) + .5)\n",
    "\n",
    "    plt.figure()\n",
    "    COLORS = 'bgrcmyk'\n",
    "    for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n",
    "        plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n",
    "\n",
    "    plt.title(\"Comparing feature reduction techniques for \" + name[0:8])\n",
    "    plt.xlabel('Reduced number of features')\n",
    "    plt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)\n",
    "    plt.ylabel('Digit classification AUC')\n",
    "    plt.ylim((0, 1))\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!! Now we have all out functions and we understand what they do. Let's try them out then. I always started making a list of all subjects I have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject = os.listdir('C:\\\\Users\\\\Ana\\\\Desktop\\\\CI\\\\Python\\\\Subjects')\n",
    "subject = np.sort(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the function. We first have to define a list with the number of features we want to keep and a number of C parameters we want to test. Then, we use our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on subject DB171120v03HT\n",
      "loading data...\n",
      "defining reduction techniques\n",
      "training gridsearch\n",
      "saving results\n"
     ]
    }
   ],
   "source": [
    "N_FEATURES_OPTIONS = [10, 20, 30, 40, 50, 60, 100]\n",
    "C_OPTIONS = [1]\n",
    "\n",
    "grid, reduction_results = grid_dim_red('C:\\\\Users\\\\Ana\\\\Desktop\\\\CI\\\\Python\\\\Subjects\\\\', subject[0], 'struct_cor', N_FEATURES_OPTIONS, C_OPTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1ddac0de6e86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m        \u001b[1;33m(\u001b[0m\u001b[1;34m'classify'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m    ])\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'param_grid' is not defined"
     ]
    }
   ],
   "source": [
    "# now we plot our results!\n",
    "plot_dimcomp(subject[0], mean_scores=reduction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can use the grid search to test everything: different classifiers, different filter cutoffs, you can use multiple scoring system at the same time! It is really loads of fun, but takes sometime to compute. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
